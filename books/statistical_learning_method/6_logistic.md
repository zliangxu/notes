逻辑斯谛回归和最大熵模型
------
[深度blog](http://www.hanlongfei.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/08/05/mle/)  
逻辑回归、最大熵模型都是用于分类的对数线性模型。
# 二项逻辑斯谛回归

## 模型  
对于分类任务，我们需要得到两个范围在$0\sim 1$之间的概率值，而线性回归的输出是$-\infty\sim +\infty$，所以需要对线性回归的输出做变换，使其值域在$0\sim1$，或者等价的，考虑将$0\sim1$映射为$-\infty\sim+\infty$。  

考虑第二种方法，首先，几率$odds=\frac{p}{1-p}$，可以将$p\in(0\sim1)$映射到$0\sim+\infty$，然后在众多非线性函数中，对数函数的定义域为正数，值域为实数，且是单调的，因此可以计算$\log(odds)=\log(\frac{p}{1-p})=logit(p)$  

$logit$变换是一一映射的变换，且存在其反变换，反变化也就得到了$sigmoid$函数
> $$\log(\frac{p}{1-p})=\theta^Tx \quad\rightarrow\quad p=\frac{exp(\theta^Tx)}{1+exp(\theta^Tx)}$$
以上也就得到了二项逻辑斯谛回归的模型

## 策略  
这里使用最大似然估计来估计二项逻辑斯谛回归的参数$\theta$。  
最大似然估计是建立在各样本间相互独立且样本满足随机抽样(可代表总体分布)下的估计方法。    
**核心思想是如果现有样本可以代表总体，那么最大似然估计就是找到一组参数使出现现有样本的可能性最大，也就是使所有观测样本的联合概率最大，由于样本相互独立，所以观测样本的联合概率可以写成各样本出现概率的连乘积。**因为单个样本不可能有$0,1$两个标签，所以化简为下式等号的右侧
> $$\prod^m_{i=1}\underbrace{P(y^{(i)}=1|x^{(i)})}_{where \, i\in m \, and \, y^{(i)}=1} \cdot \underbrace{P(y^{(i)}=0|x^{(i)})}_{where \, i\in m \, and \, y^{(i)}=0}=\prod^m_{i=1} P(y^{(i)}=1|x^{(i)})^{y(i)} \cdot P(y^{(i)}=0|x^{(i)})^{1-y(i)}$$
通常，上式称为似然函数$\ell(\theta)$。(它的形式和概率相同，这里称为似然，是因为它在这里的使用场景，是在已知观测结果时，用于求解，使联合概率最大化时的模型参数$\theta$，[看wiki似然函数](https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0))  

然而上面的似然函数形式是非凸的，可以使用对数函数使其转换为凸函数(局部最小即全局最小的函数)，也就得到对数似然函数。

对于线性回归，如果误差项满足正态分布，那么最小二乘估计的均方误差损失可以由最大似然估计推导出来。

## 算法
模拟退火、单纯形算法、梯度下降算法。

# 最大熵模型
[Maxent theme page](https://homepages.inf.ed.ac.uk/lzhang10/maxent.html)  
主要用于NLP  
熵是混乱度的表示，不要把鸡蛋放在一个篮子里就是最大熵原理。----吴军<数学之美>  
思想是，满足约束条件，且没有更多信息的情况下，那些不确定的部分都是"等可能的"。最大熵原理通过熵的最大化来表示等可能度，"熵"是一个可优化的数值指标。
## 模型
## 策略
## 算法

## 拉格朗日对偶
主要把限制条件带入优化函数中()；其次把原始问题转换为对偶问题，方便求解最优值。
- 原始问题(原始最优化问题)
> $$\begin{aligned} & _{x\in R^n}^{min} &f(x)  \\
&s.t. &c_i(x) \leqslant0, & i=1,2,...,k \\
& &h_j(x)=0, & j=1,2,...l\end{aligned} $$     
引进拉格朗日函数得
> $$L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k \alpha_ic_i(x)+\sum_{j=1}^{l} \beta_jh_j(x)$$
其中，$x\in R^n$，$\alpha_i$、$\beta_j$是拉格朗日乘子，$\alpha_i \geqslant 0$。
- 对偶问题
- 原始问题与对偶问题的关系
- KKT条件
## 改进的迭代尺度算法(Improved Iterative Scaling,IIS)
## 拟牛顿法

## tips
- s.t. : subject to 服从于
- 几率(odds)：一个事件发生的概率与不发生的概率的比值$odds=\frac{p}{1-p}$。
- 对数几率(log odds): $logit(p)=\log\frac{p}{1-p}$



