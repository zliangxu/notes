## tips
1. 隐藏层的激活函数应该使用非线性函数，由此在神经网络中引入非线性。  
    线性函数的组合仍是线性函数，这样包含多层隐藏层不能够增加表达能力。

## 从自我学习到深度网络
无监督特征学习步骤有时被称为预训练。  
微调是指对无监督特征学习加上使用学习到的特征进行有监督学习的结构进行监督训练做网络参数调整。  

## 深度学习概览
我们可以找到的一些函数，这些函数可以使用k层神经网络简洁的表示出来(简洁是指网络的规模(隐藏层神经单元的数目)与输入单元数目呈多项式关系)。而如果使用k-1层神经网络，则需要使用与输入单元数目呈指数关系的隐藏层单元数目，否则无法表达这些函数。
- 训练深度网络的难度
    数据量需求大：难以收集大数量数据  
    局部极致问题：代价函数是高度非凸的，优化算法容易陷入局部最小值，而不能求得全局最小值。  
    梯度弥散问题：随着梯度前向传播，梯度会迅速减小，导致前面几层的网络得不到训练。
- 逐层贪婪训练方法。

## 栈式自编码算法
每一个自编码网络包括两层，第一层叫做编码，第二层的输出又是整个自编码网络的输入，所以叫做解码。

## 微调多层自编码算法
这里要对误差的定义要清楚，参看neural networks and deep learning中的反向传递算法。