# 目标函数的理解
[blog](https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138)
[翻译](https://www.jiqizhixin.com/articles/2017-12-11-5)

## introduction
是为了培养对神经网络目标函数的直觉
- 为什么要使用MSE及cross entropy log loss作为目标函数？为什么规则项有作用？  

分类问题中，数据真实分布$p$与网络输出分布$q$之间的cross entropy log loss
> $$CE=-\sum_{x} p(x) \log q(x)$$
回归问题中，数据真实值$y_i$与网络回归值$\tilde{y}_i$的MSE误差
> $$MSE=\frac{1}{N} \sum_{i=1}^{N}(y_i-\tilde{y}_i)^2$$
- 神经网络的预测概率是很容易出错的，例如，面对adversarial examples时，即便出错，置信度也往往很高。
- 如何从概率的角度解释神经网络模型，人们往往以概率理解神经网络的输出，那么神经网络自身与目标函数是否也有概率解释。

## 监督学习
在监督学习问题中，数据集$D$是由数据对$(x,y)$组成，其中$x$是样本，$y$是与之对应的标签。然后试着建立模型表示它们之间的分布
> $$p(y|x,\theta)$$
这样的模型叫做判别式模型(也叫条件模型？)，给定一个观测数据$x$，模型会输出它的概率分布，用于估计$y$的值。

判别式模型分类问题中，可学习参数集合$\theta$被用于定义从$x$到范畴分布概率的映射，判别式模型会输出$N$个概率，表示$x$属于各个类的概率。然而$x$只会属于一个类，模型输出的概率分布就表示了模型的不确定性。  

判别式模型回归问题中，对于观测数据$x$，模型只会输出一个值，并没有每一个可能值的概率分布。判别式模型回归问题的一个输出值，有些误导，但实际上它的输出是高斯分布的，而输出值就是高斯分布的均值，高斯分布的方差并没有建模，或者对于所有$x$，方差被选为常数。对于样本$x$，模型输出均值的同时也输出方差的模型，提供的信息量更多，因为它可以表示对$x$的不确定度。 

对有训练样本的地方，一个模型对它的输出应该有较高的置信度，而对于没有训练数据的地方，模型的输出应该有较高的不确定度。  

其它概率模型如高斯过程(Gaussian Process)在回归问题中，很好的对不确定性进行了建模，然而当要同时对均值和方差进行建模时，判别式模型会倾向于过度自信？

高斯过程能够通过方差很好的量化不确定度，但是在大训练集时，优点并不能很好的体现出来。在有大量训练数据的区域，模型的置信区间(由方差确定)很小，而训练数据很少的区域，模型的置信区间很大。

## 与神经网络的联系
当神经网络用于训练分类、回归任务时，范畴分布、高斯分布的参数通过神经网络建模。  
当使用极大似然估计来确定$\theta$的值，概念就清晰些
> $$\begin{aligned} \theta ^{MLE} &= {\arg\max}_{\theta} \log p(Y|X,\theta) \\
&= {\arg\max}_{\theta} \log \prod_{i=1}^{N} p(y_i|x_i,\theta) \\
&= {\arg\max}_{\theta} \sum_{i=1}^{N} \log p(y_i|x_i,\theta)\end{aligned}$$
$p(y|x,\theta)$表示模型参数为$\theta$时，模型输出真实label的概率。
- 交叉熵   
最大化范畴分布的对数概率与最小化数据真实分布与估计分布的交叉熵有关
- MSE  
最大化高斯分布的对数概率与最小化数据真实分布与估计分布的MSE有关。
？？如何公式重写成关联的状态？？？

## 最大化后验概率
如果神经网络可以解释为概率模型，为什么会出现错的概率估计，被adversarial example愚弄？又为什么需要大量训练数据？  

意识到神经网络是用MLE来估计参数的，而MLE很容易对训练数据过拟合，并且想得到好的结果就需要大量训练数据。而机器学习的目标是找到一个模型，它能够很好的解释训练数据，能很好的泛化到测试数据，而对与训练数据相差很大的数据不确定。

最大化后验概率(MAP)是一个解决过拟合问题的很好的概率模型，那么MAP是如何影响目标函数的？  

与MLE类似，MAP也可以写成神经网络里的目标函数。本质上，MAP是在$\theta$上假设一个先验分布的同时，在给定训练数据上，寻找最大化概率的$\theta$值。
> $$\begin{aligned} \theta^{MAP} &= {\arg\max}_{\theta} \log p(\theta|X,Y) \\
&\approx {\arg\max}_{\theta} \log p(Y|X,\theta) \cdot p(\theta) \quad\text{这个贝叶斯公式是如何推理的？}\end{aligned}$$
最后一个等号后的前半部分与MLE相同，是要模型尽量能够解释训练数据，后半部分是要模型符合假设的$\theta$先验分布。

L2规则项是假设的高斯分布参数，让$\theta$尽量小；L1规则项是假设的拉普拉斯分布参数，让大部分参数为0。

## 完整的贝叶斯方法

## tips
resp.regression表示？